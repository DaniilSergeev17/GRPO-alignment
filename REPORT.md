# Отчет

## 1) GRPO alignment на датасете gsm8k 
В данном задании мне было предложено обучить модель Qwen/Qwen2.5-0.5B-Instruct на датасете gsm8k с помощью GRPO, чтобы модель научилась выдавать ответ в структурированном виде: ```'<reasoning>...</reasoning><answer>...</answer>'```

Идея в том, чтобы модель перед ответом научилась "думать" и складывать свои размышления между ```'<reasoning>'``` и ```'</reasoning>'``` токенами

GRPO - метод, использующий rule-based reward функции вместо обученной reward-модели. Поэтому для реализации этой идеи мне требовалось написать rule-based reward функции 

Я поставил перед собой цель - исследовать поведение модели, при разных rule-based reward функциях

### Код
> Основные скрипты - grpo_train.py (классический пайплайн GRPO alignment'а) и sft_grpo_train.py (веса lora weights подгружаются из папки lora_after_sft (которую надо создать и положить туда веса, которые получены после выполнения блокнота sft_before_grpo.ipynb и лежат в папке lora_after_sft))

> Основные блокноты (папка notebooks) - grpo_train.ipynb (классический пайплайн GRPO alignment'а, но в виде блокнота для удобства), sft_before_grpo.ipynb (SFT модели перед этапом GRPO) и change_len_prompt_bert.ipynb (обучение ModernBERT для задачи регрессии количества токенов в ответе по входному вопросу)

> Результаты выполнения скриптов (логи) лежат в папке logs 

> Результаты выполнения блокнотов хранятся в самих блокнотах в папке notebooks 

### Полезные переменные
```bash
system_prompt = """
  Respond in the following format:
  <reasoning>
  ...
  </reasoning>
  <answer>
  ...
  </answer>
""" 
```

### Подходы и результаты
1) **baseline**: В качестве baseline я взял предобученную модель Qwen/Qwen2.5-0.5B-Instruct и просто прогнал на тестовом сете датасета gsm8k, подавая system_prompt 
> Результаты: Accuracy -> 0.23654283548142532, Right format percent -> 0.0

> Выводы: Модель не слушается промпта, так как не следует формату ответа

2) **GRPO с двумя reward'ами**: Далее я решил проверить, как модель поведет себя, если провести GRPO alignment стадию с двумя rule-based reward функциями (как в оригинальной статье DeepSeekMath, исключая reward для языка) -> strict_format_reward_func (reward функция, которая выдает 1.0 за строгое соответствие формату промпта и 0.0 иначе) и exact_match_reward_func (reward функция, которая выдает 1.0 за строгое совпадение ответа модели внутри ```<answer>ANSWER</answer>``` блока с числовым таргетом)
> Результаты: Accuracy -> 0.2312357846853677, Right format percent -> 0.9893858984078847

> Анализ обучения (логи в logs/grpo_run_2_reward.log): Качество, относительно бейзлайна немного упало, но модель очень хорошо научилась генерировать ответ в правильном формате (как в system_prompt). Из интересного: моделька поняла, что лучше всего генерировать ответы длины порядка 115 токенов, strict_format_reward достиг 0.98, exact_match_reward достиг 0.20, значение реварда с эпохи 0.34 до 1.0 практически не менялось, что говорит о слабом фидбеке от среды (чувствуется нехватка reward функций и их простота)

> Выводы: Вроде бы модель научилась почти идеально (98%) отвечать в правильном формате, но качество генерации немного просело (хотя у нас был reward на это). Кажется, что стоит улучшить фидбек от среды и попробовать получить качество генерации лучше 

3) **GRPO с четырьмя reward'ами**: Подумав, что модели "не хватает" награды (значение награды за правильный формат уперлось около 0.95, а значение правильного ответа оставалось низким, что может быть вызвано нехваткой параметров и квантизацией 4 бит и, как следствие, модель переставала учиться (так как ей больше неоткуда взять награду)) и модель могла бы более плавно адаптироваться к правильному ответу и формату, помимо strict_format_reward_func и exact_match_reward_func я добавил: isnumber_reward_func (модель получает награду 0.25, если ее ответ - это число) и soft_format_reward_func (модель получает награду 0.5 за присутствие каждого из блоков: ```<reasoning>...</reasoning>``` и ```<answer>...</answer>```)
> Результаты: Accuracy -> 0.2486732373009856, Right format percent -> 0.9264594389689158

> Анализ обучения (логи в logs/grpo_run_4_reward_baseline.log): Как видно по логам, сначала модель генерировала длинные последовательности, затем генерировала очень короткие последовательности (порядка 10 токенов), пока с эпохи 0.46 не поняла, что можно получать награду за правильную структуру ответа (соответствующие награды strict_format_reward_func и soft_format_reward_func тоже начали резко расти) и => повышать количество токенов в ответе (с 50 дошло до среднео значения в 110 токенов). exact_match_reward повышался в процессе обучения и в конце достил значения порядка 0.23. is_number_reward достаточно быстро достиг своего максимума (порядка 0.25). reward рос на всем этапе обучения, что хорошо. На всем периоде обучения KL-дивергенция вела себя нестабильно (то держалась около 1, то прыгала до 33, 3300), на последней итерации KL достигает 68, что не очень хороший знак, так как модель сильно отклоняется от reference (исходной) модели, но при этом награда растет и этот факт не учитывается (возможно, beta слишком маленькая и модель не обращает внимание на KL-дивергенцию, либо reward функции позволяют модели давать слишком много награды из-за своей простоты)

> Выводы: GRPO обучение с четырьмя reward'ами показало: увеличение точности относительно бейзлайна и модель стала "слушаться" промпта с форматом вывода в примерно 93% случаев, чего мы и хотели добиться

4) **GRPO с четырьмя reward'ами, но с новыми значениями**: Далее я решил использовать 4 reward'а как бейзлайн и попробовал поперебирать значения награды в exact_match_reward_func (так как именно с этим ревардом у модели были проблемы (она набирала около 0.23 из 1.0))
> **Reward 1.5 за правильный ответ (было 1.0) (логи в logs/grpo_run_4_reward_1_5_exact_match.log)**: Так как я увидел, что reward_exact_match довольно низкий (около 0.23 при максимуме 1.0), то решил увеличить награду за правильный ответ, в надежде, что модель станет больше внимания уделять генерации правильного ответа (так как дается больше награды и таким образом происходит балансировка с наградой format, потому что там максимальная награда - 2 за все format reward функции). Результаты: Accuracy -> 0.2395754359363154, Right format percent -> 0.9552691432903715. Вывод: к моему удивлению accuracy понизилась, а процент ответов в правильном формате увеличился до 95%. Из интересного: модель вышла на генерацию порядка 120 токенов, exact_match_reward вырос до 0.36 (с 0.23 прирост -> 56.6%, что выше 50%, на которые я увеличил reward (с 1.0 до 1.5), следовательно, вероятно, это действие немного повлияло на поведение модели в сторону большего reward'а), остальные reward'ы стабильно остаются почти на максимуме

> **Reward 0.8 за правильный ответ (было 1.0) (логи в logs/grpo_run_4_reward_0_8_exact_match.log)**: Стало интересно, а что будет если понизить reward. Результаты: Accuracy -> 0.058377558756633814, Right format percent -> 0.0. Вывод: немного не очевидно, что понижение reward'а на такое небольшое значение даст такой плохой результат. Очевидный вывод: я создал сильный дисбаланс с reward'ами за соответствие формату ответа

> **Reward 1.2 за правильный ответ (было 1.0) (логи в logs/grpo_run_4_reward_1_2_exact_match.log)**: Стало интересно, а что будет если немного понизить reward относительно 1.5. Результаты: Accuracy -> 0.045489006823351025, Right format percent -> 0.0. Вывод: очень интересный результат, так как дисбаланс наград здесь работать не должен (как было показано ранее с наградой 1.0 и 1.5 качество было адекватным). Из интересного: модель сошлась к генерации в среднем всего 10 токенов, вообще не уловила структуру ответа (значение награды 0.0), exact_match_reward всего 0.06, isnumber_reward - максимальный (0.25), а самое интересное - soft_format_reward ровно 0.5, что дает предположить (да, я забыл сделать пару debug выводов), что модель уловила только генерацию токенов ```<answer>...</answer>``` (так как isnumber_reward - максимальный, а он максимальный только когда есть число в ответе и, самое важное - это число располагается между answer-токенами)

5) **SFT + GRPO 4 rewards**: Вдохновившись подходом авторов статьи DeepSeekMath, я решил, что стоит попробовать перед GRPO стадией "познакомить" модель с данными и реализовал такой пайплайн: Сначала учим модель (lora адаптер) SFT на трейн сете датасета gsm8k (авторегрессивно учиться генерировать ответ в формате: ```<reasoning>\n{raw_reasoning}\n</reasoning>\n<answer>\n{clear_answer}\n</answer>```, где raw_reasoning и clear_answer - соответствующие значения из таргета в трейн сете (в gsm8k они разделены '####'))
> Результаты SFT (логи и код доступны в notebooks/sft_before_grpo.ipynb): Accuracy in right format -> 0.0, Accuracy in wrong format -> 0.24791508718726307, Right format percent -> 0.0 (Accuracy in wrong format - когда модель не смогла сгенерировать ответ в правильном формате, качество смотрится по последнему числу, которое сгенерировала модель (в данной задаче - это неплохая эвристика))
 
> Анализ обучения SFT: Было сделано всего 500 шагов обучения, но accuracy правильных ответов выросла до значения, сопоставимого с accuracy после GRPO-обучения, но модели не хватило 500 шагов, чтобы уловить структуру ответа

> Результаты SFT + GRPO (логи в logs/sft_grpo_4_reward.log): Accuracy -> 0.047763457164518575, Right format percent -> 0.0
 
> Выводы: У модели strict_format_reward = 0.0 почти на всем периоде обучения, что говорит о несоответствии строгой структуре ответа (как и SFT модель) (возможно, потому что lora веса SFT модели сделали смещение от исходной модели в сторону генерации ответа в неправильном формате). Но интересно, что soft_format_reward держится в районе 0.5, а isnumber_reward в районе 0.25 (максимум), а isnumber_reward считается только если в ответе есть ```<answer>...</answer>``` => модель практически всегда возвращает числовой ответ между answer токенами, что говорит о том, что, возможно, эта комбинация reward'ов перевешивает остальные reward'ы

6) **Future Works**: Можно играться бесконечно, но обучение стоит денег, поэтому я оставил это на будущее:
> Сделать reward функцию, которая поощряет правильный ответ не только внутри ```<answer>ANSWER</answer>``` блока (exact_match_reward_func), но и в неструктурированном ответе

> Сначала SFT на задаче генерации кода, затем GRPO на gsm8k (этот подход показал значительное увеличение качества на gsm8k в статье DeepSeekMath) 

> Сделать больше шагов в SFT, а потом переобучить GRPO с инициализацией lora весов из обученной SFT модельки (мне жалко бедную А100, а colab'а не хватает на двухэтапное обучение)

## 2) HARD: предсказание длины ответа (в токенах) по входному запросу 

### Подходы и результаты
1) **BERT for predicting length of an answer given a question**: Первое, что я решил попробовать - обучить модель ModernBERT-base (просто улучшенная версия BERT и всех его сородичей) на задаче регрессии, где input - это вопрос, а target - это длина ризонинга внутри ```<reasoning>...</reasoning>``` блока. На этапе инференса мы прогоняем вопрос через ModernBERT и получаем ожидаемую длину ризонинга в токенах (reasoning_length), затем изменяем system_prompt, добавляя внутрь ризонинг блока такую строчку: ```<reasoning>reasoning for {reasoning_length} tokens</reasoning>``` и ожидаем от модели генерацию ризонинга в reasoning_length токенов. Модель для генерации ответа я взял предобученную и не делал GRPO
> Результаты: Reasoning length stats: Expected -> 75.0, Actual -> 35.0. Accuracy in right format -> 0.0, Accuracy in wrong format -> 0.21607278241091737, Right format percent -> 0.0 

> Анализ результатов: В среднем bert выдает значение в 75 токенов, а модель генерирует 35 токенов на ризонинге, это говорит о том, что модель не думает о длине в токенах (она не училась на такой задаче). Также модель стала генерировать менее качественные ответы, чем бейзлайн

> Вывод: Промпт про длину генерации "сбивает с толку" модель

2) **Reward for reasoning length**: Следующее, что я решил реализовать - добавить reward функцию, которая сравнивает количество токенов в ризонинг блоке, который сгенерировала модель и количество токенов в таргете (у нас есть ризонинг в таргете - это все, что до '####' в answer), нормируя это в [0.0, 1.0], где 0.0 - различие в количестве токенов >= количество токенов в таргете, а 1.0 - идеальное совпадение количества токенов. Вот эта reward функция: ```reward = max(0, 1 - abs(длина_ответа - таргет_длина) / таргет_длина)```. И затем сделать на этом реварде (+4 базовых реварда из пункта 3)) GRPO
> Результаты: Accuracy -> 0.25018953752843065, Right format percent -> 0.9825625473843821

> Скорость инференса: 1319/1319 [24:56<00:00,  1.13s/it] (бейзлайн подход с 4 наградами обрабатывал весь тест за 12 минут)

> Анализ результатов (логи в logs/grpo_run_5_rewards_1_0_for_length.log): Качество улучшилось как генерации ответа, так и генерации структуры ответа! Как видим, скорость инференса уменьшилась вдвое, что говорит о том, что модель стала тратить в среднем больше токенов на генерацию и поэтому, "подумав" дольше, модель стала выдавать более качественные и структурированные (по формату) ответы. Модель сошлась к средней длине ответа - 135 токенов, что в среднем больше, чем в бейзлайне. Из интересного: is_number_reward не дошел до своего максимума (остановился на 0.21), target_length_reward достиг 0.66, то есть модель не научилась идеально предсказывать длину ответа в токенах, как видно по логам, она и не пытается идти дальше, чем 0.67, возможно, модели не хватает параметров 

> Вывод: Так как датасет gsm8k состоит из математических задач, где нужно хорошо подумать, скорость инференса уменьшилась вдвое. Однако, есть предположения (небезосновательные), что если мы это все обучим на датасете с вопросами различной сложности (Как дела?; Реши задачу тысячелетия: ...; и так далее), то скорость инференса будет более стабильной и будет варьироваться от вопроса к вопросу (так как сейчас модель сходится к средней длине ответа - 135 токенов, что больше бейзлайна)

3) **L1: LCPO** Затем я наткнулся на статью L1, где вводят метод LCPO - это GRPO, но с функцией награды, которая поощряет правильную длину генерации (когда я реализовывал пункт 2) я не знал про эту статью). В L1 вводятся два rule-based reward'а: Первый (exact length) - ```I(y=y_gold) - alpha * abs(n_gold - n_y)```, где y=y_gold - проверка на правильность ответа, а n_gold-n_y - разность длин таргет ответа и сгенерированного моделью ответа, alpha - контроллирует влияние разности длин на reward, я выкинул первую часть (так как у меня уже есть reward, контроллирующий качество ответа) и оставил только ```- alpha * abs(n_gold - n_y)```), этот ревард нужен для того, чтобы смотреть не только на качество ответа, но и на соответствие длины ответа длине таргета. Второй (max) - ```I(y=y_gold) * clip(alpha * (n_gold - n_y) + beta, 0, 1)```, про этот ревард подробнее написал ниже 

3.1) **LCPO: max**
> Результаты: Accuracy -> 0.05155420773313116, Right format percent -> 1.0
 
> Скорость инференса: 1319/1319 [03:49<00:00,  5.75it/s]. 1319 сэмплов всего за 4 минуты! Из логов видно, что модель сошлась к генерации в среднем 20 токенов (что довольно мало для математических задач и ризонинга на них)

> Анализ результатов (логи в logs/lcpo_5_rewards_max_reward.log): Такое поведение модели абсолютно ожидаемо, ведь из формулы: ```clip(alpha * (target_len - generated_len) + beta, 0, 1)``` видно, что чем меньше токенов сгенерировано, тем лучше, а из логов видно, что lcpo_max_length_reward достиг 0.99-1.0, что объясняет такое поведение (модель просто поняла, что должна генерировать очень маленькие последовательности, чтобы ревард клипнулся в 1.0). Из интересного: strict_format_reward и soft_format_reward достигают максимума (скорее всего модель научилась генерировать только токены формата и немного токенов сверху (так как isnumber_reward тоже максимален, значит и ответ тоже генерируется), а для ризонинга остается очень мало токенов, что и сказывается на качестве генерации)

> Вывод: LCPO max стоит использовать в сочетании с ревардами на качество генерации, без этого модель должна вообще всегда генерировать нулевые по длине ответы. Параметр alpha стоит сделать поменьше (у меня 0.03), чтобы модель сильно не обращала на это внимание и не переобучалась под генерацию очень коротких ответов. Для задач, где нужен глубокий ризонинг (например, как наша задача - gsm8k математика) такой метод обучения не подойдет 

3.2) **LCPO: exact length**
> Результаты: 

> Скорость инференса:
 
> Анализ результатов (логи в logs/lcpo_5_rewards_exact_len_reward.log):

> Вывод: 

4) **Future Works**:
> Попробовать подход с бертом на моделях с бОльшим числом параметров (которые будут более менее слушаться ограничений в промпте)

> Поперебирать награды (посмотреть, как они ведут себя друг с другом) и найти лучшую комбинацию

> Обучить LCPO max и LCPO exact length вместе 

> Придумать более качественную reward функцию на выбор длины ответа в токенах 